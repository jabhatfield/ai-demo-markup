= Zoo Chatbot deep dive
:navtitle: Deep dive
:icons: font
:xrefstyle: short

This deep dive outlines the main concepts of the Zoo Chatbot inner workings.

== Natural Language Processing

*Natural Language Processing* (*NLP*) is the leverage of computing to understand language. It involves a variety of
linguistic datasets, or _corpora_, along with associated algorithms that transform this data into language models.
Zoo Chatbot implements https://opennlp.apache.org[OpenNLP,window=_blank] to perform the following NLP tasks, which are
explained in following sections:

* <<_tokenization>>
* <<_part_of_speech_tagging>>
* <<_lemmatization>>
* <<_categorisation>>

== Tokenization

Zoo Chatbot tokenizes a message into its constituent parts, such as words and punctuation marks, for subsequent
<<_part_of_speech_tagging>>. Words are predominantly segmented by white space, with punctuation marks potentially attached
in the case of abbreviations. Tokenization decisions are based on a pre-trained <<_maximum_entropy,maximum entropy>>
model of tokenization data.

== Part-of-speech tagging

The <<_tokenization,tokens>> are tagged with
https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html[Penn Treebank,window=_blank]
part-of-speech labels to indicate word classes and punctuation marks. The choice of tag depends on the context in
terms of surrounding tokens. Tagging decisions are based on a pre-trained <<_maximum_entropy,maximum entropy>> model
of tag data.

== Lemmatization

The part-of-speech tags allow tokens to be lemmatized, meaning words are converted into their base form. This allows
variants of the same word to be recognised. Lemmatization decisions are based on a pre-trained statistical model
of lemmatization data. The training algorithm is based on a
https://grzegorz.chrupala.me/papers/phd-single.pdf[PhD dissertation by Grzegorz Chrupala,window=_blank]
and performs two main functions:

* *Specialised suffixation:* many word inflections affect the suffix, for example _big_ can become _bigger_ or _biggest_.
The lemma _big_ is formed by traversing from the end of the inflected word and performing a series of insertions
and deletions.

* *Generalised prefixation and suffixation:* word forms inapplicable to specialised suffixation are traversed from both
the beginning and the end to find the longest common substring, or _stem_. For example, the Polish words for _hard_
and _hardest_ are _trudny_ and _najtrudniejszy_ respectively, and they share the stem of _trudn_. The lemma _trudny_
is formed through a series of insertions and deletions.

== Categorisation

inc bag of words - words as numbers

== Maximum entropy

The maximum entropy algorithm finds a probability distribution that maximises entropy, within constraints dictated by
the data. Consider a dice roll as an analogy. A dice roll has high entropy, meaning that it's unpredictable, with a
uniformly spread out probability distribution (see <<regular-dice-diagram>>). Empirical training data will indicate
this uniform distribution and therefore impose no real constraint, since the model already exhibits maximum entropy.

.Regular dice with high entropy
[#regular-dice-diagram]
image::regular-dice-green.png[]

Suppose the dice is weighted in favour of rolling a 6. A naive algorithm might model the dice as always rolling a 6 and
therefore indicate zero entropy (see <<unrealistic-weighted-dice-diagram>>). A lack of entropy results in failure to
represent outliers; however, as illustrated in <<regular-dice-diagram>>, too much entropy results in a vague model of
randomness. A realistic solution is somewhere between the two approaches.

.Unrealistic weighted dice with zero entropy
[#unrealistic-weighted-dice-diagram]
image::unrealistic-weighted-dice-green.png[]

Consider the same weighted dice modelled with maximum entropy. The algorithm will maximise entropy by assigning importance
to outliers (values 1-5). However, the outliers will be considered less significant than the more common value of 6 because
the algorithm is constrained by the data, which favours a value of 6. The model is specific enough to show trends yet
generic enough to include less common outcomes (see <<realistic-weighted-dice-diagram>>).

.Realistic weighted dice with low entropy
[#realistic-weighted-dice-diagram]
image::realistic-weighted-dice-green.png[]

== Learn more

* See the xref:tutorial/chat-tutorial.adoc[] for a practical guide of operations.
* See the xref:intro-component::api-spec.adoc[] for the full specification.